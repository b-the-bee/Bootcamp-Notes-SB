#data-science #data-engineering

## Overview
- High level definition of ETL
- What problems can ETL solve
- What happens in each stage?
- ETL vs ELT

### Learning Objectives

- Understand what happens in the extract, load and transform phases
- Implement an ETL pipeline using Python and SQL
- Understand when you might want to use ETL or ELT

---

## What is ETL?

- **Extract, Transform and Load**
- A way to move data from multiple sources and save it in a single target location
- Extract: Data is pulled from the relevant sources
- Transform: The data is changed to make it suitable for the target system
- Load: The transformed data is then saved into the target system


### Problems with data

Often keeping track of data across many systems is complex when...

- Events occur at different times - One source sends data by the minute, another aggregates data and sends it hourly - Data from events may not arrive in the order that they happened and so will need reconciliation later
- Data formats vary - The logs from the backend come in CSV and the logs from the front end come in JSON but you want to make direct comparisons, only run the same query once etc.
- There's too much data to process when extracting - This will only occur with big data (can be solved with out of the box solutions e.g. AWS Firehose)
- Systems are separated logically or physically - You take data from the logs generated by your backend, which is hosted on AWS and you also take data from a third-party API. You want to bring the data from these sources into one place so they can back end queried together.
- Unpredictable extra load affects the source system - If impacted during a set time, might make it difficult to process data immediately transform data, can be ETL at another point

### Consequences

This can lead to unreliable systems and therefore unreliable data.

How many possible consequences of unreliable data could you think of as a group? - Databases can be under high load if the source is being accessed at multiple times for different uses

Some possible consequences of unreliable data:

- Financial institutions: incorrect balances, lost transactions - Financial transactions - there need to be systems in place to warn if information is not up to date or if data has become corrupted. People's bank balances depend on this!
- Scientific research: false conclusions - Scientific research - might be less time sensitive but just as important that data is not lost and, if it is, there are alerts in place. A researcher's theories become harder to prove or disprove without this. Research often informs public policy on vital areas e.g. health, law, trade and education. Incorrect data could have a negative knock-on effect, creating policies informed by bad data.
- Retail: lost orders and deliveries - Retail orders - needs to accommodate rapidly changing usage patterns (e.g. online supermarkets after an announcement of lockdown) and keep track of people's orders - or at least be alerted if people's orders have got lost. This is especially important for those shielding or with other vulnerabilities as they rely more heavily on being able to make online purchases for delivery.
- Loss of reputation

## ETL Stages

![[Pasted image 20240621094159.png]]

--- 

## Extract
### Where does the data come from?

Data is sourced from one or more systems, for example:

- Server logs
- Third parties
- Another database


### What format will the data be in?

This data will come in a variety of formats:

- JSON
- XML
- CSV
- Parquet (a column-oriented data storage format)
- Databases
- Other formats such as log files

### When will the data be extracted?

This usually happens when...

- A timed event occurs i.e. daily, hourly etc
- A database trigger event occurs
- A manual process is run

Notes: What other events might cause the need for extraction?

- A transaction (think apps like Monzo that send a notification right after you've made a purchase)

- A news event (think the automatic trading algorithms that process news feeds to decide whether to buy or sell)

- natural environment changes - so if data from earthquake sensors, dam pressure sensors and the like go above a particular threshold


### How do you extract the data?

You can extract the data using different transfer methods, for example:

- Secure File Transfer Protocol (SFTP) is a network protocol that provides file access, transfer and management.
- Network Shares are when a computer resource is made available from one host to other hosts on a network
- Object Stores like Amazon S3


### How do you know the data is formatted correctly?

During extraction it is important to validate that this data is acceptable for passing to the transformation stage.

We can do this by matching predictable patterns, schemas or by running hash functions against the data.

If the data is invalid then appropriate alerting and/or metrics should be produced to inform customers and downstream systems.

---
## Transform

Rules or functions are applied to the extracted data to perform any of the following

- Normalisation
- Cleaning the data to a specific format or encoding
- Selecting specific columns/fields
- Performing calculations on fields e.g. 1000ms to 1s

Notes: Cleaning:

- Removing rows with null values in cells that you care about

Selecting specific columns:

- Time based exclusions, for tax reporting you only care about data between particular windows

Performing calculations:

- Maybe you want to find the average of a particular column, let's say average goals scored per season by a particular player over a season

### More transformations...

- Sorting
- Deduplicating data
- Grouping or Aggregating
- Joining data together with other datasets

Are there any other useful transformations you can think of?

Notes: Sorting

- An international newspaper might want to sort their subscription information to have a visual rundown of where most subscribers live, from highest to lowest

Grouping / aggregating:

- Counting customers by gender

Joining:

- Mapping relationship between flooding data sets and crop yield data sets

---

## Load

Load the data into the target storage solution, often another relational database system.

Often existing data is overwritten or updated with cumulative information on a daily, weekly, or monthly basis.

Complex systems can maintain a history and audit trail of all changes to the data loaded in the data warehouse.

---
## ETL vs ELT

### What is ELT?

- ELT stands for Extract, Load, Transform
- The processes are the same, but their order has changed

Notes: Little question for engagement re the second bullet point

Q: Now that the order has changed, what does this mean for the Transform stage's location? Answer: Now happens once it's been loaded and is inside the target system.


## Advantages of ELT

- More flexibility in querying source data
- Easier to understand relationship between raw and transformed data
- Potential cost savings

Notes: Flexibility What happens if you want to run a different transformation (e.g. new SQL query) on your source data with an ETL pipeline? You'd have to write the new query and then extract _all_ of the source data again in order to perform the new transformation and get your insights. With a massive system this could be very expensive in both time and cost. In an ELT system you already have all your raw source data available, no need to extract it, so you just run the new query and get your results

Transparency ELT can make it easier for everyone to understand the relationship between the extracted and transformed data as it's all in once place. In an ETL pipeline the source data is in a different system, which reduces the ease of access and ability to compare with transformed data.

- Business analysts e.g. For example let's say a business analyst wants to delve deeper into why they're getting a particular set of results. It's much easier for them to make comparisons between the raw source data with the transformed data when they're in the same place.
- Data Engineer e.g. Similarly, it's easier for an engineer to get see how the data that flows through their pipeline is being used and potentially make improvements, fix bugs etc.

Savings Typically in an ELT pipeline the 'Load' location is in the cloud. This means that the computing power required for the transform stage can be scaled up and down according to a businesses needs. E.g. A highly seasonal tourist business can just pay for what they need, when they need it, rather than having to invest in an on-premises fixed size system that they're not using to its full capacity most of the time


- Data protection - removing sensitive data before loading
- Can save on storage costs - removing large unused files before loading

Notes: Data protection If an industry has tight regulations on what sensitive data can be stored where, it's likely an ETL pipeline is preferable as it's easier to guarantee that sensitive information from raw data can be scrubbed during the transform stage, before it's loaded into the target system

Potential storage savings If your source data contains large files (e.g. images) that you know you're never going to need, then it makes sense to not pay to store them in your target system - with ETL you can just ignore these files during the transform phase.


## Examples

---

### ETL Example

A real estate property company allows users to search for houses.

Each house can be accessed via their website using the path `/property-12345`.

Every time a page is accessed a record is stored in the `property_view` table against the `property_id` .

That table looks something like this...

```sql
+-------------+---------------+---------+---------------+
| property_id |   timestamp   | browser |  ip_address   |
+-------------+---------------+---------+---------------+
|       12345 | 1580894343687 | Chrome  | 182.22.109.13 |
+-------------+---------------+---------+---------------+
```

Once per day the property page view counts for the previous day are extracted from the main application database, and inserted into a staging table in the data warehouse.

The query looks something like this...

```sql
TRUNCATE TABLE warehouse_db.property_view_stage;

INSERT INTO warehouse_db.property_view_stage
SELECT * FROM main_db.property_view
WHERE timestamp >= 1580860800000
AND timestamp <= 1580947200000;
```

Notes: Possible question to engage learners: why do we truncate first? If staging data is permanently stored elsewhere, then it's just wasted space storing it here as well as in the target system

Stretch: Where do these numbers come from? Is someone having to update them manually? Would it be better if they were automated and calculated, then could just refer to them as variables. Could introduce concept of magic numbers Currently looks like magic numbers no variables, so are they inserted manually?... https://en.wikipedia.org/wiki/Magic_number_(programming)#Unnamed_numerical_constants


---

### ETL Example

The data is transformed using a `GROUP BY` aggregation and inserted into another staging table.

The query looks something like this...

```sql
TRUNCATE TABLE warehouse_db.page_view_daily_aggregation_stage;

INSERT INTO warehouse_db.page_view_daily_aggregation_stage
SELECT DATE(FROM_UNIXTIME(timestamp/1000)) as property_view_date,
property_id, COUNT(1) as property_view_count
FROM warehouse_db.property_view_stage
GROUP BY property_view_date, property_id;
```

That table looks something like this...

```sql
+--------------------+-------------+---------------------+
| property_view_date | property_id | property_view_count |
+--------------------+-------------+---------------------+
| 2020-02-05         |       12345 |                  32 |
| 2020-02-05         |       67890 |                  21 |
+--------------------+-------------+---------------------+
```


This data is then loaded to the final location where it can be used in reports by estate agents to their customers.

The query looks something like this...

```sql
INSERT INTO warehouse_db.page_view_daily_aggregation
SELECT * FROM warehouse_db.page_view_daily_aggregation_stage
```

The final aggregated data looking something like this...

```sql
+--------------------+-------------+---------------------+
| property_view_date | property_id | property_view_count |
+--------------------+-------------+---------------------+
| 2020-02-05         |       12345 |                  32 |
| 2020-02-06         |       12345 |                  15 |
| 2020-02-05         |       67890 |                  32 |
| 2020-02-06         |       67890 |                  21 |
+--------------------+-------------+---------------------+
```

This data could be further aggregated or used to produce insightful customer reports...

Can you think of any other examples of how this data could provide insight?

Notes: Property view rate

- If the amount of times your property is viewed is below the average, you could look at what keywords you're using to describe it and think about some search engine optimisation measures

Click through rate:

- If this is below average you might want to think about things like what profile image you're using for the property, whether you would want to add more information like floor plans or simply increase the number of photographs

A user could tweak these things one by one to see if their numbers start to change

If all of a user's metrics are above average but the property still isn't selling, then they could start to think about what's not being recorded with the current system and whether that might provide some more insight into the behaviour they're seeing..

For example:

- There's no current way to record if visitors think the price is reasonable
- There's no comparison of larger trends outside of page views - e.g. does the time of year impact on how many properties are sold vs. when people are more window-shopping? Is there a national slump in purchases that relates to wider economic conditions?

